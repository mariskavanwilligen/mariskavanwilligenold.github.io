Title: SUBiNN Simulation Study
Date: 2021-06-05 11:14
Tags: SUBINN, Simulation study, Power
Slug: simulation-study
Authors: Mariska van Willigen
Summary: A Simulation Study to Investigate the Feature Selection Performance of a Stacked Uni- and Bivariate Nearest Neighbors Classifier

## Simulation study to study SUBiNN model

SUBiNN is a Stacked Uni- and Bivariate Nearest Neighbors classifier with kNN classifiers as base-learners and a Lasso feature selection as meta-learner. The
aim of this present research is to estimate the power, type I error and the false
discovery rate of the SUBiNN model to find interactions and main effects in
the data. 

See my previous post for more information about the SUBiNN model [Click here!](https://mariskavanwilligen.github.io/subinn-model.html)

## Simulation study set-up

This simulation study makes use of a full factorial 3-by-3-by-2 design
where systematically varied sample sizes and effect sizes are used in the data
generating process. Additionally, varied number of neighbors k are used in the
kNN classifiers.

![](/images/Thesis/Thesis4b.png)

For each of the 100 replications, SUBiNN is fitted to a data set
with 10 uniformly distributed features with one main effect and two interaction
effects. The power, type I error and false discovery rate will be estimated to
measure the feature selection performance in every condition. The stability of
the results is examined in a follow-up study with one data set.

![](/images/Thesis/Thesis6.png)

## Data generation process

For each replication, a new data set of 10 independent distributed
features in the range -1 till +1 is generated with a specific sample size and effect size. With these features two interactions effects (X2-X3 and X5-X6) and
one main effecg X7 are defined. The data generating process is given by:

![](/images/Thesis/formula.png)

Function S returns a 1 when the statement is fullfilled and a -1 otherwise. With
the probabilities, observations from a binominal distribution are sampled. The
main effect of X7 is defined as a step function. The data points of X7 with a
value between -1 and -0.5 or between 0 and 0.5 are returning a 1 by function S.
All the other values of X7 are returning a -1.


The two interaction effects can be described as Four Blocks and Gaussian
ordination. These two interactions are nonlinear and described by Yuan et al.
(2019). Additionally, the Gaussian ordination is also used in a simulation study
by Gul et al. (2016). Both interactions are pure interactions in the sense that
no linear regression model will be able to classify a data point correctly if there
is one pure interaction effect and an outcome variable. The interactions are
expected to be informative and not the linear main effects of the variables used
in the interactions.


The interactions before sampling from a binomial distribution are visualized
in Figure below. The left plot shows the Four Blocks interaction effect between X2
and X3 and the right plots shows the Gaussian ordination interaction effect
between X5 and X6. The two different colors represent the two different classes
of outcome vector y.

![](/images/Thesis/Thesis13.png)


## Results 

The results of the simulation study shows that a bigger effect size and sample
size leads to an increased ability to identify the interaction and main effects.
Furthermore, the different values of the number of neighbors k did not have a
significant impact on the power. However, the number of neighbors did have an
impact on the type I error, a higher number of neighbors will result in a lower
type I error. Additionally, the interaction effects were overall easier identifiable
by the SUBiNN model compared with the main effect. When looking specifically
at the two interaction effects, the Four Blocks interaction had a higher estimated
power than the Gaussian ordination.


The high false discovery rates indicate that not all of the uninformative base-learners were filtered out. 
Mainly base-learner X7 had a low estimated power
and was often selected as an interaction effect (Figure 3d-i). Especially base-learners X5-X7 and X6-X7 
were frequently selected by the model. It seems that
the SUBiNN model occasionally tries to capture the information in bivariate
pairs instead of univariate base-learners.


![](/images/Thesis/Thesis14.png)  

From the heatmaps and the boxplots, it can be observed that a few uninformative base-learners are often selected by the model. A (N x P) predictor matrix
X has P features and therefore R = (P + P(P * 1)=2) different base-learners
with every possible uni- and bivariate combination of the features. The uninformative base-learners that were not filtered out often consist of two informative
features or a combination of one informative feature and one uninformative
feature. When looking on feature level, the model is able to filter out all the
uninformative features. In the heatmaps (Figure 3d - 3i) a clear pattern is visible and features X2, X3, X5, X6 and X7 are correctly considered as important
features. 

So to conclude, the model finds it difficult to filter out all the uninformative base-learners, but not to filter out all the uninformative features. 
To filter out more uninformative base-learners, it might be useful to take a look at
the median or mode of all the coeficients given by Lasso after repeated 10-fold
cross validation. In the end, the number of positive median or mode coeficients
can be counted. This will filter out more uninformative base-learners compared
with one time 10-fold cross validation and converting the coeficients to zeros
and ones to do a count.


![](/images/Thesis/Thesis12.png)

## Conclusion
The results of this simulation study show that a bigger effect size and sample
size leads to an increased ability to identify the interaction and main effects.


Furthermore, the different values of neighbors did not have a significant impact on the power. Although, the number of neighbors did have an impact
on the type I error, a higher number of neighbors will result in a lower type
I error. In all the conditions, the false discovery rates are quite high, mean-
ing that of all the selected base-learners, there are still a few uninformative
base-learners selected.  



Based on this simulation study, it is
strongly recommended to have a sample size of at least 500 and the number
of neighbors set to 10 or the square root of N with repeated cross-validation to identify main
effects and interactions with a SUBiNN model.



