Title: Machine Learning with Spark
Date: 2021-06-14 15:36
Tags: ML, Spark, sklearn
Slug: ml-spark
Authors: Mariska van Willigen
Summary: Using Spark Machine Learning to predict house prices in Boston

<img src="/images/mlspark/spark.png" alt="drawing" width="400"/>

## Using Spark Machine Learning to predict prices for houses in Boston

The very famous and well known boston housing dataset is used for playing around with Spark ML. The goal is to set up a pipeline with SparkML to predict the house prices.
The dataset consists of the following features:

> **CRIM** - per capita crime rate by town
>
> **ZN** - proportion of residential land zoned for lots over 25,000 sq.ft.
>
> **INDUS** - proportion of non-retail business acres per town
>
> **RM** - average number of rooms per dwelling
>
> **AGE** - proportion of owner-occupied units built prior to 1940
>
> **DIS** - weighted distances to five Boston employment centres
>
> **CHAS** - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
>
> **NOX** - nitric oxides concentration (parts per 10 million)
>
> **RAD** - index of accessibility to radial highways
>
> **TAX** - full-value property-tax rate per $10,000
>
> **PTRATIO** - pupil-teacher ratio by town
>
> **B** - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
>
> **LSTAT** - % lower status of the population
>
> **MEDV** - Median value of owner-occupied homes in $1000's


## Loading the dataset
First the dataset is going to be loaded in. `header=True` is used, so the colnames are already there. Furthermore, `inferSchema=True` will let Spark decide on each coltype. It is also possible to specify this manually by giving a schema. The extra index column and rows with missing values are being dropped.
The outcome column `medv` is renamend as `label` this is important to use the different SparkML functions. Life is going to be more easier if the outcome column is called `label` and the other features are the `feature_cols`.
```js
boston = spark.read.csv('Boston.csv', header=True, inferSchema=True)

boston = (boston.dropna()
         .drop('_c0')
         .withColumnRenamed('medv', 'label'))

feature_cols = ['crim',
                'zn',
                'indus',
                'chas',
                'nox',
                'rm',
                'age',
                'dis',
                'rad',
                'tax',
                'ptratio',
                'black',
                'lstat']

boston.limit(5).toPandas()
```
![](/images/mlspark/display.png)


## Split in trainset and testset
Next step is to split the dataset in a train and test set wtih ratio 80/20, the random seed is set to archieve the same split and therefore same result each time the code will run.

```js
train_df, test_df = boston.randomSplit([0.8, 0.2], seed=707)

print("Train count:", train_df.count())
print('Test count:', test_df.count())
```
![](/images/mlspark/counts.png)

# Fitting a simpel Random forest model without grid search and cross validation

Build ML workflows as a chain of Transformers and Estimators.

> **DataFrame**: Boston data (e.g., input, features, labels, predictions).
>
> **Transformer**: Transforms one DataFrame into another DataFrame.
>
> **Estimator**: Fits on a DataFrame and produces a Transformer.
>
> **Parameter**: Common API for specifying parameters.

The `VectorAssembler` is a transformer that combines a given list of columns into a single vector column and `MinMaxScaler` transforms a dataset of Vector rows, rescaling each feature to a specific range (often `[0, 1]`). Then, a random forest regressor is used to model the data. Ofcourse, before specifyng the model, more preprocessing steps can be defined if necessary.
After the pipeline has been set up and the model has been fitted, the predictions can be derived with `.transform()` and the MSE and R2 can be calculated with the predictions and the real values.

```js
assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_vector')
scaler = MinMaxScaler(inputCol='features_vector', outputCol='features')
regressor = RandomForestRegressor()

pipeline = Pipeline(stages=[assembler, scaler, regressor])
with_predictions = pipeline.fit(train_df).transform(test_df)

evaluator = RegressionEvaluator()
print("MSE:", evaluator.evaluate(with_predictions, {evaluator.metricName: "mse"}))
print("R2:", evaluator.evaluate(with_predictions, {evaluator.metricName: "r2"}))
```
![](/images/mlspark/withoutgrid.png)

# Hyperparameter tuning with Grid search and crossvalidation
A gridsearch can be performed to find the optimal number of trees out of 1, 3, 50 and 100 number of trees. Also 5 fold crossvalidation is added.
```js
assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_vector')
scaler = MinMaxScaler(inputCol='features_vector', outputCol='features')
regressor = RandomForestRegressor()

pipeline = Pipeline(stages=[assembler, scaler, regressor])

num_trees = [1, 3, 50, 100]
grid = (
   ParamGridBuilder()
   .addGrid(regressor.numTrees, num_trees)
   .build()
)
cv = (
   CrossValidator()
   .setEstimator(pipeline)
   .setEvaluator(RegressionEvaluator(metricName='mse'))
   .setEstimatorParamMaps(grid)
   .setNumFolds(5)
)
```
The cv_model now contains the best model based on the MSE. With this bestmodel again predictions can be made and evaluated to get the MSE and the R2.
```js
cv_model = cv.fit(train_df)

with_predictions = cv_model.bestModel.transform(test_df)
with_predictions.select(["features", "label", "prediction"]).limit(5).toPandas()

evaluator = RegressionEvaluator()
print("MSE:", evaluator.evaluate(with_predictions, {evaluator.metricName: "mse"}))
print("R2:", evaluator.evaluate(with_predictions, {evaluator.metricName: "r2"}))
```
![](/images/mlspark/withgrid.png)

For now, this blogpost showed what I learned about SparkML and how to create a simple model with a pipeline, grid search and crossvalidation with SparkML instead of Sklearn. Ofcourse, the prediction accuracy and the MSE & R2 can be improved by implementing more advanced model techniques. It could also be very beneficial to do more preprocessing. 
