Title: Shapley value
Date: 2021-10-29 15:20
Modified: 2021-10-29 15:20
Tags: Shapley value, Machine learning, Game Theory
Slug: shapley-value
Authors: Mariska van Willigen
Summary: Short introduction about Shapley value and how they can be used to explain machine learning models

## Introduction
The quality of your ML model is for a big part depended on the explainability of your model. Shapley values can help you to explain and interpret your model.
Shapley values, a method from coalitional game theory, explain how fairly distributed the “payout” is among the features. With Shapley values you can explain the output based on the input.

The Shapley value is the average marginal contribution of a feature value across all possible coalitions.
The interpretation of the Shapley value for feature value j is: The value of the j-th feature contributed ϕj to the prediction of this particular instance compared to the average prediction for the dataset. The Shapley value works for both classification (if we are dealing with probabilities) and regression.

> Let's have a look!

## Data Set Information:

The dataset is about Portuguese "Vinho Verde" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

This dataset can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.

Attribute Information:

Input variables (based on physicochemical tests) *(For more information, read [Cortez et al., 2009])*:
- fixed acidity
- volatile acidity
- citric acid
- residual sugar
- chlorides
- free sulfur dioxide
- total sulfur dioxide
- density
- pH
- sulphates
- alcohol

Output variable (based on sensory data):
- quality (score between 0 and 10)


``` js
# Some code example
import pyspark
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
sc = SparkContext('local')
spark = SparkSession(sc)

wine_data = spark.read.csv('data/winequality-red.csv', header=True, inferSchema=True)

wine_data.limit(5).toPandas()
```
![](/images/shapley/data_view.png)

## Model fitting

``` js
wine_data = wine_data.toPandas()
y = wine_data['quality']
X = wine_data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol']]

# a simple linear model
model = sklearn.linear_model.LinearRegression()
model.fit(X, y)
```

``` js
import shap
shap_values = shap.TreeExplainer(model).shap_values(X)
shap.summary_plot(shap_values, X, plot_type="bar")
```
![](/images/shapley/tree.png)

The SHAP value plot can further show the positive and negative relationships of the predictors with the target variable. 

``` js
import matplotlib.pyplot as plt
f = plt.figure()
shap.summary_plot(shap_values, y)
``` 
![](/images/shapley/tree2.png)

This plot is made of all the dots in the train data. It demonstrates the following information:
Feature importance: Variables are ranked in descending order.
- **Impact:** The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.
- **Original value:** Color shows whether that variable is high (in red) or low (in blue) for that observation.
- **Correlation:** A high level of the “alcohol” content has a high and positive impact on the quality rating. The “high” comes from the red color, and the “positive” impact is shown on the X-axis. Similarly, we will say the “volatile acidity” is negatively correlated with the target variable.

You may ask how to show a partial dependence plot. The partial dependence plot shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 2001). It tells whether the relationship between the target and a feature is linear, monotonic or more complex. In order to create a dependence plot you do:

``` js
shap.dependence_plot('alcohol', shap_values, X)
``` 

The function automatically includes another variable that your chosen variable interacts most with. The following plot shows there is an approximately linear and positive trend between “alcohol” and the target variable, and “alcohol” interacts with “sulphates” frequently:

![](/images/shapley/shap3.png)

``` js
import numpy as np

# Get the predictions and put them with the test data.
X_output = y.copy()
X_output.loc[:,'predict'] = np.round(model.predict(X_output),2)

# Randomly pick some observations
random_picks = np.arange(1,330,50) # Every 50 rows
S = X_output.iloc[random_picks]

# Initialize your Jupyter notebook with initjs(), otherwise you will get an error message.
shap.initjs()

# Write in a function
def shap_plot(j):
    explainerModel = shap.TreeExplainer(model)
    shap_values_Model = explainerModel.shap_values(S)
    p = shap.force_plot(explainerModel.expected_value, shap_values_Model[j], S.iloc[[j]])
    return(p)

shap_plot(0)
``` 
![](/images/shapley/shap_plot.png)

This plot describes:
- **The output value:** is the prediction for that observation (the prediction of the first row in Table B is 6.20).
- **The base value:** The original paper explains that the base value E(y_hat) is “the value that would be predicted if we did not know any features for the current output.” In other words, it is the mean prediction, or mean(yhat). You may wonder why it is 5.62. This is because the mean prediction of Y_test is 5.62. You can test it out by Y_test.mean() which produces 5.62.
- **Red/blue:** Features that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue.
- **Alcohol:** has a positive impact on the quality rating. The alcohol content of this wine is 11.8 (as shown in the first row of Table B) which is higher than the average value 10.41. So it pushes the prediction to the right.
- **pH:** has a negative impact on the quality rating. A lower than the average pH (=3.26 < 3.30) drives the prediction to the right.
Sulphates: is positively related to the quality rating. A lower than the average Sulphates (= 0.64 < 0.65) pushes the prediction to the left.


Dataset retrieved from: *P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.*