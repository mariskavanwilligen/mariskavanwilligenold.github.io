Title: Stacked Uni- and Bivariate Nearest Neighbors (SUBiNN)
Date: 2021-05-25 13:44
Tags: SUBINN, KNN, Stacking, Classification
Slug: subinn-model
Authors: Mariska van Willigen
Summary: SUBiNN is a Stacked Uni- and Bivariate Nearest Neighbors classifier.

## SUBiNN

SUBiNN is a Stacked Uni- and Bivariate Nearest Neighbors classifier with kNN
classifiers as base-learners and a Lasso feature selection as meta-learner. 

![](/images/Thesis/Thesis1.png)

The SUBiNN model consists of two layers, namely the base-learner layer and the meta-learner layer. Both layers are using 10-fold cross-validation. Breiman (1996) noted that cross-validated base-learner predictions are important to avoid overfitting.

## Base-learner layer

In the base-learner layer, a kNN classifier is used on every possible uni- and bivariate combination of the features within the data set to keep the dimension- ality low and to avoid the curse of high dimensionality. So, a (N x P) predictor matrix X with N number of observations and P features has a response vector y of length N and will have R = (P + P (P − 1)/2) different kNN classifiers, also called base-learners. The output of the base-learner layer will consists of a (N x R) matrix Z with the probabilities of classifying a data point as classs 1. This matrix is the output of the base-learner layer and is used as input for the meta-learner layer.


## Meta-learner layer

The meta-learner layer consists of a Lasso feature selection and is trained on matrix Z. With Lasso, each base-learner will receive a coefficient. These coefficients will represent the importance of the base-learners. Lasso adds a L1 penalty to the sum of squared residuals to reduce overfitting (Tibshirani, 1996). Consider the N by R matrix Z and the response vector y of length N. One observation of Z can be denoted by Zij where i stands for the ith row and j for the jth base-learner with i=1,....N and j=1,....R. The Lasso loss function to be minimized is:

![](/images/Thesis/Thesis15.png)

The impact of the L1 penalty is determined by the value of λ. When λ is big enough, some coefficients will be penalized so much that they will shrink to zero. Base-learners with a positive β coefficient will be kept in the model and the other base-learners are filtered out. This has the advantage of reducing the number of predictors in the model (Tibshirani, 1996). In a perfect scenario, all the selected base-learners are informative base-learners.

So, the first step in this meta-learner layer is to estimate the optimal λ with 10-fold cross-validation. A linear model with Lasso regularization is fitted 10 times, once for each fold. For each λ an averaged prediction error and standard deviation is calculated over the folds. The λ with the lowest error is chosen as the optimal estimated λ. With this estimated λ, a linear model with Lasso regularization is fitted on the complete data. At this point, every base-learner has received a β coefficient. The base-learners with a positive coefficient are considered as the base-learners that are selected by the model.

![](/images/Thesis/Thesis3.png)