<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Mariska van Willigen - Posts</title><link href="/" rel="alternate"></link><link href="/feeds/posts.atom.xml" rel="self"></link><id>/</id><updated>2021-06-14T15:36:00+02:00</updated><entry><title>Machine Learning with Spark</title><link href="/ml-spark.html" rel="alternate"></link><published>2021-06-14T15:36:00+02:00</published><updated>2021-06-14T15:36:00+02:00</updated><author><name>Mariska van Willigen</name></author><id>tag:None,2021-06-14:/ml-spark.html</id><summary type="html">&lt;p&gt;Using Spark Machine Learning to predict house prices in Boston&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img src="/images/mlspark/spark.png" alt="drawing" width="400"/&gt;&lt;/p&gt;
&lt;h2&gt;Using Spark Machine Learning to predict prices for houses in Boston&lt;/h2&gt;
&lt;p&gt;The very famous and well known boston housing dataset is used for playing around with Spark ML. The goal is to set up a pipeline with SparkML to predict the house prices.
The dataset consists of the following features:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CRIM&lt;/strong&gt; - per capita crime rate by town&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZN&lt;/strong&gt; - proportion of residential land zoned for lots over 25,000 sq.ft.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;INDUS&lt;/strong&gt; - proportion of non-retail business acres per town&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RM&lt;/strong&gt; - average number of rooms per dwelling&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AGE&lt;/strong&gt; - proportion of owner-occupied units built prior to 1940&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DIS&lt;/strong&gt; - weighted distances to five Boston employment centres&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CHAS&lt;/strong&gt; - Charles River dummy variable (1 if tract bounds river; 0 otherwise)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOX&lt;/strong&gt; - nitric oxides concentration (parts per 10 million)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RAD&lt;/strong&gt; - index of accessibility to radial highways&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TAX&lt;/strong&gt; - full-value property-tax rate per $10,000&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PTRATIO&lt;/strong&gt; - pupil-teacher ratio by town&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;B&lt;/strong&gt; - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LSTAT&lt;/strong&gt; - % lower status of the population&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MEDV&lt;/strong&gt; - Median value of owner-occupied homes in $1000's&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Loading the dataset&lt;/h2&gt;
&lt;p&gt;First the dataset is going to be loaded in. &lt;code&gt;header=True&lt;/code&gt; is used, so the colnames are already there. Furthermore, &lt;code&gt;inferSchema=True&lt;/code&gt; will let Spark decide on each coltype. It is also possible to specify this manually by giving a schema. The extra index column and rows with missing values are being dropped.
The outcome column &lt;code&gt;medv&lt;/code&gt; is renamend as &lt;code&gt;label&lt;/code&gt; this is important to use the different SparkML functions. Life is going to be more easier if the outcome column is called &lt;code&gt;label&lt;/code&gt; and the other features are the &lt;code&gt;feature_cols&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;spark&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;read&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Boston.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;inferSchema&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;boston&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
         &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_c0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;withColumnRenamed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;medv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="nx"&gt;feature_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;crim&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;zn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;indus&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;chas&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;nox&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;rm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;age&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;dis&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;rad&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;tax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;ptratio&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;lstat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="nx"&gt;boston&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;5&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="/images/mlspark/display.png"&gt;&lt;/p&gt;
&lt;h2&gt;Split in trainset and testset&lt;/h2&gt;
&lt;p&gt;Next step is to split the dataset in a train and test set wtih ratio 80/20, the random seed is set to archieve the same split and therefore same result each time the code will run.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;train_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;test_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;boston&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;randomSplit&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nx"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;707&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Train count:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;train_df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Test count:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;test_df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="/images/mlspark/counts.png"&gt;&lt;/p&gt;
&lt;h1&gt;Fitting a simpel Random forest model without grid search and cross validation&lt;/h1&gt;
&lt;p&gt;Build ML workflows as a chain of Transformers and Estimators.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;DataFrame&lt;/strong&gt;: Boston data (e.g., input, features, labels, predictions).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt;: Transforms one DataFrame into another DataFrame.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimator&lt;/strong&gt;: Fits on a DataFrame and produces a Transformer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Parameter&lt;/strong&gt;: Common API for specifying parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;code&gt;VectorAssembler&lt;/code&gt; is a transformer that combines a given list of columns into a single vector column and &lt;code&gt;MinMaxScaler&lt;/code&gt; transforms a dataset of Vector rows, rescaling each feature to a specific range (often &lt;code&gt;[0, 1]&lt;/code&gt;). Then, a random forest regressor is used to model the data. Ofcourse, before specifyng the model, more preprocessing steps can be defined if necessary.
After the pipeline has been set up and the model has been fitted, the predictions can be derived with &lt;code&gt;.transform()&lt;/code&gt; and the MSE and R2 can be calculated with the predictions and the real values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;assembler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;VectorAssembler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inputCols&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;feature_cols&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;outputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;features_vector&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;MinMaxScaler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;features_vector&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;outputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;regressor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;RandomForestRegressor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;stages&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;assembler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scaler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;regressor&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nx"&gt;with_predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;train_df&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;test_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;evaluator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;RegressionEvaluator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MSE:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;with_predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;metricName&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;R2:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;with_predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;metricName&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;r2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="/images/mlspark/withoutgrid.png"&gt;&lt;/p&gt;
&lt;h1&gt;Hyperparameter tuning with Grid search and crossvalidation&lt;/h1&gt;
&lt;p&gt;A gridsearch can be performed to find the optimal number of trees out of 1, 3, 50 and 100 number of trees. Also 5 fold crossvalidation is added.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;assembler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;VectorAssembler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inputCols&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;feature_cols&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;outputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;features_vector&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;MinMaxScaler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;features_vector&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;outputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;regressor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;RandomForestRegressor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;stages&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;assembler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;scaler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;regressor&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="nx"&gt;num_trees&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nx"&gt;grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="nx"&gt;ParamGridBuilder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
   &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;addGrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;regressor&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;numTrees&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;num_trees&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;build&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="nx"&gt;CrossValidator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
   &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setEstimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setEvaluator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;RegressionEvaluator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;metricName&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
   &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setEstimatorParamMaps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setNumFolds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The cv_model now contains the best model based on the MSE. With this bestmodel again predictions can be made and evaluated to get the MSE and the R2.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;cv_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;cv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;train_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;with_predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;cv_model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bestModel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;test_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;with_predictions&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;select&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;features&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;prediction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]).&lt;/span&gt;&lt;span class="nx"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;5&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;toPandas&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nx"&gt;evaluator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;RegressionEvaluator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MSE:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;with_predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;metricName&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;R2:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;with_predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;metricName&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;r2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="/images/mlspark/withgrid.png"&gt;&lt;/p&gt;
&lt;p&gt;For now, this blogpost showed what I learned about SparkML and how to create a simple model with a pipeline, grid search and crossvalidation with SparkML instead of Sklearn. Ofcourse, the prediction accuracy and the MSE &amp;amp; R2 can be improved by implementing more advanced model techniques. It could also be very beneficial to do more preprocessing. &lt;/p&gt;</content><category term="Posts"></category><category term="ML"></category><category term="Spark"></category><category term="sklearn"></category></entry><entry><title>SUBiNN Simulation Study</title><link href="/simulation-study.html" rel="alternate"></link><published>2021-06-05T11:14:00+02:00</published><updated>2021-06-05T11:14:00+02:00</updated><author><name>Mariska van Willigen</name></author><id>tag:None,2021-06-05:/simulation-study.html</id><summary type="html">&lt;p&gt;A Simulation Study toInvestigate the Feature SelectionPerformance of a Stacked Uni-and Bivariate Nearest NeighborsClassifier&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Simulation study to study SUBiNN model&lt;/h2&gt;
&lt;p&gt;SUBiNN is a Stacked Uni- and Bivariate Nearest Neighbors classifier with kNN classifiers as base-learners and a Lasso feature selection as meta-learner. The
aim of this present research is to estimate the power, type I error and the false
discovery rate of the SUBiNN model to find interactions and main effects in
the data. &lt;/p&gt;
&lt;p&gt;See my previous post for more information about the SUBiNN model &lt;a href="https://mariskavanwilligen.github.io/subinn-model.html"&gt;Click here!&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Simulation study set-up&lt;/h2&gt;
&lt;p&gt;This simulation study makes use of a full factorial 3-by-3-by-2 design
where systematically varied sample sizes and effect sizes are used in the data
generating process. Additionally, varied number of neighbors k are used in the
kNN classifiers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis4b.png"&gt;&lt;/p&gt;
&lt;p&gt;For each of the 100 replications, SUBiNN is fitted to a data set
with 10 uniformly distributed features with one main effect and two interaction
effects. The power, type I error and false discovery rate will be estimated to
measure the feature selection performance in every condition. The stability of
the results is examined in a follow-up study with one data set.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis6.png"&gt;&lt;/p&gt;
&lt;h2&gt;Data generation process&lt;/h2&gt;
&lt;p&gt;For each replication, a new data set of 10 independent distributed
features in the range -1 till +1 is generated with a specific sample size and effect size. With these features two interactions effects (X2-X3 and X5-X6) and
one main effecg X7 are defined. The data generating process is given by:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/formula.png"&gt;&lt;/p&gt;
&lt;p&gt;Function S returns a 1 when the statement is fullfilled and a -1 otherwise. With
the probabilities, observations from a binominal distribution are sampled. The
main effect of X7 is defined as a step function. The data points of X7 with a
value between -1 and -0.5 or between 0 and 0.5 are returning a 1 by function S.
All the other values of X7 are returning a -1.&lt;/p&gt;
&lt;p&gt;The two interaction effects can be described as Four Blocks and Gaussian
ordination. These two interactions are nonlinear and described by Yuan et al.
(2019). Additionally, the Gaussian ordination is also used in a simulation study
by Gul et al. (2016). Both interactions are pure interactions in the sense that
no linear regression model will be able to classify a data point correctly if there
is one pure interaction effect and an outcome variable. The interactions are
expected to be informative and not the linear main effects of the variables used
in the interactions.&lt;/p&gt;
&lt;p&gt;The interactions before sampling from a binomial distribution are visualized
in Figure below. The left plot shows the Four Blocks interaction effect between X2
and X3 and the right plots shows the Gaussian ordination interaction effect
between X5 and X6. The two different colors represent the two different classes
of outcome vector y.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis13.png"&gt;&lt;/p&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;The results of the simulation study shows that a bigger effect size and sample
size leads to an increased ability to identify the interaction and main effects.
Furthermore, the different values of the number of neighbors k did not have a
significant impact on the power. However, the number of neighbors did have an
impact on the type I error, a higher number of neighbors will result in a lower
type I error. Additionally, the interaction effects were overall easier identifiable
by the SUBiNN model compared with the main effect. When looking specically
at the two interaction effects, the Four Blocks interaction had a higher estimated
power than the Gaussian ordination.&lt;/p&gt;
&lt;p&gt;The high false discovery rates indicate that not all of the uninformative base-learners were filtered out. 
Mainly base-learner X7 had a low estimated power
and was often selected as an interaction effect (Figure 3d-i). Especially base-learners X5-X7 and X6-X7 
were frequently selected by the model. It seems that
the SUBiNN model occasionally tries to capture the information in bivariate
pairs instead of univariate base-learners.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis14.png"&gt;  &lt;/p&gt;
&lt;p&gt;From the heatmaps and the boxplots, it can be observed that a few uninformative base-learners are often selected by the model. A (N x P) predictor matrix
X has P features and therefore R = (P + P(P * 1)=2) different base-learners
with every possible uni- and bivariate combination of the features. The uninformative base-learners that were not filtered out often consist of two informative
features or a combination of one informative feature and one uninformative
feature. When looking on feature level, the model is able to filter out all the
uninformative features. In the heatmaps (Figure 3d - 3i) a clear pattern is visible and features X2, X3, X5, X6 and X7 are correctly considered as important
features. &lt;/p&gt;
&lt;p&gt;So to conclude, the model finds it difficult to filter out all the uninformative base-learners, but not to filter out all the uninformative features. 
To filter out more uninformative base-learners, it might be useful to take a look at
the median or mode of all the coeficients given by Lasso after repeated 10-fold
cross validation. In the end, the number of positive median or mode coeficients
can be counted. This will filter out more uninformative base-learners compared
with one time 10-fold cross validation and converting the coeficients to zeros
and ones to do a count.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis12.png"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The results of this simulation study show that a bigger effect size and sample
size leads to an increased ability to identify the interaction and main effects.&lt;/p&gt;
&lt;p&gt;Furthermore, the different values of neighbors did not have a significant impact on the power. Although, the number of neighbors did have an impact
on the type I error, a higher number of neighbors will result in a lower type
I error. In all the conditions, the false discovery rates are quite high, mean-
ing that of all the selected base-learners, there are still a few uninformative
base-learners selected.  &lt;/p&gt;
&lt;p&gt;Based on this simulation study, it is
strongly recommended to have a sample size of at least 500 and the number
of neighbors set to 10 or the square root of N with repeated cross-validation to identify main
effects and interactions with a SUBiNN model.&lt;/p&gt;</content><category term="Posts"></category><category term="SUBINN"></category><category term="Simulation study"></category><category term="Power"></category></entry><entry><title>Introduction</title><link href="/my-super-post.html" rel="alternate"></link><published>2021-04-12T10:20:00+02:00</published><updated>2010-12-05T19:30:00+01:00</updated><author><name>Mariska van Willigen</name></author><id>tag:None,2021-04-12:/my-super-post.html</id><summary type="html">&lt;p&gt;Who am I?&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hi EVERYONE!!! :) !!!, I wil introduce myself here, blaa blaa blaa My name is Mariska and I might update this blog or maybe never and just make it for fun. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Who knows?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;Mariskas_objects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;b&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;small cute dog&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Very happy Mariska :)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;small cute dog&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Cheesefondue&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;OMGGGGG YESSSSSS :D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Images&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Minion" src="https://octodex.github.com/images/minion.png"&gt;&lt;/p&gt;</content><category term="Posts"></category><category term="First Post"></category><category term="Introduction"></category><category term="Publishing"></category></entry></feed>