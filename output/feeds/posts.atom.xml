<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Mariska van Willigen - Posts</title><link href="/" rel="alternate"></link><link href="/feeds/posts.atom.xml" rel="self"></link><id>/</id><updated>2021-06-05T11:14:00+02:00</updated><entry><title>SUBiNN Simulation Study</title><link href="/simulation-study.html" rel="alternate"></link><published>2021-06-05T11:14:00+02:00</published><updated>2021-06-05T11:14:00+02:00</updated><author><name>Mariska van Willigen</name></author><id>tag:None,2021-06-05:/simulation-study.html</id><summary type="html">&lt;p&gt;A Simulation Study toInvestigate the Feature SelectionPerformance of a Stacked Uni-and Bivariate Nearest NeighborsClassifier&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Simulation study to study SUBiNN model&lt;/h2&gt;
&lt;p&gt;SUBiNN is a Stacked Uni- and Bivariate Nearest Neighbors classifier with kNN classifiers as base-learners and a Lasso feature selection as meta-learner. The
aim of this present research is to estimate the power, type I error and the false
discovery rate of the SUBiNN model to find interactions and main effects in
the data. &lt;/p&gt;
&lt;p&gt;See my previous post for more information about the SUBiNN model &lt;a href="https://mariskavanwilligen.github.io/subinn-model.html"&gt;Click here!&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Simulation study set-up&lt;/h2&gt;
&lt;p&gt;This simulation study makes use of a full factorial 3-by-3-by-2 design
where systematically varied sample sizes and effect sizes are used in the data
generating process. Additionally, varied number of neighbors k are used in the
kNN classifiers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis4b.png"&gt;&lt;/p&gt;
&lt;p&gt;For each of the 100 replications, SUBiNN is fitted to a data set
with 10 uniformly distributed features with one main effect and two interaction
effects. The power, type I error and false discovery rate will be estimated to
measure the feature selection performance in every condition. The stability of
the results is examined in a follow-up study with one data set.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis6.png"&gt;&lt;/p&gt;
&lt;h2&gt;Data generation process&lt;/h2&gt;
&lt;p&gt;For each replication, a new data set of 10 independent distributed
features in the range -1 till +1 is generated with a specific sample size and effect size. With these features two interactions effects (X2-X3 and X5-X6) and
one main effecg X7 are defined. The data generating process is given by:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/formula.png"&gt;&lt;/p&gt;
&lt;p&gt;Function S returns a 1 when the statement is fullfilled and a -1 otherwise. With
the probabilities, observations from a binominal distribution are sampled. The
main effect of X7 is defined as a step function. The data points of X7 with a
value between -1 and -0.5 or between 0 and 0.5 are returning a 1 by function S.
All the other values of X7 are returning a -1.&lt;/p&gt;
&lt;p&gt;The two interaction effects can be described as Four Blocks and Gaussian
ordination. These two interactions are nonlinear and described by Yuan et al.
(2019). Additionally, the Gaussian ordination is also used in a simulation study
by Gul et al. (2016). Both interactions are pure interactions in the sense that
no linear regression model will be able to classify a data point correctly if there
is one pure interaction effect and an outcome variable. The interactions are
expected to be informative and not the linear main effects of the variables used
in the interactions.&lt;/p&gt;
&lt;p&gt;The interactions before sampling from a binomial distribution are visualized
in Figure below. The left plot shows the Four Blocks interaction effect between X2
and X3 and the right plots shows the Gaussian ordination interaction effect
between X5 and X6. The two different colors represent the two different classes
of outcome vector y.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis13.png"&gt;&lt;/p&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;The results of the simulation study shows that a bigger effect size and sample
size leads to an increased ability to identify the interaction and main effects.
Furthermore, the different values of the number of neighbors k did not have a
significant impact on the power. However, the number of neighbors did have an
impact on the type I error, a higher number of neighbors will result in a lower
type I error. Additionally, the interaction effects were overall easier identifiable
by the SUBiNN model compared with the main effect. When looking specically
at the two interaction effects, the Four Blocks interaction had a higher estimated
power than the Gaussian ordination.&lt;/p&gt;
&lt;p&gt;The high false discovery rates indicate that not all of the uninformative base-learners were filtered out. 
Mainly base-learner X7 had a low estimated power
and was often selected as an interaction effect (Figure 3d-i). Especially base-learners X5-X7 and X6-X7 
were frequently selected by the model. It seems that
the SUBiNN model occasionally tries to capture the information in bivariate
pairs instead of univariate base-learners.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis14.png"&gt;  &lt;/p&gt;
&lt;p&gt;From the heatmaps and the boxplots, it can be observed that a few uninformative base-learners are often selected by the model. A (N x P) predictor matrix
X has P features and therefore R = (P + P(P * 1)=2) different base-learners
with every possible uni- and bivariate combination of the features. The uninformative base-learners that were not filtered out often consist of two informative
features or a combination of one informative feature and one uninformative
feature. When looking on feature level, the model is able to filter out all the
uninformative features. In the heatmaps (Figure 3d - 3i) a clear pattern is visible and features X2, X3, X5, X6 and X7 are correctly considered as important
features. &lt;/p&gt;
&lt;p&gt;So to conclude, the model finds it difficult to filter out all the uninformative base-learners, but not to filter out all the uninformative features. 
To filter out more uninformative base-learners, it might be useful to take a look at
the median or mode of all the coeficients given by Lasso after repeated 10-fold
cross validation. In the end, the number of positive median or mode coeficients
can be counted. This will filter out more uninformative base-learners compared
with one time 10-fold cross validation and converting the coeficients to zeros
and ones to do a count.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/images/Thesis/Thesis12.png"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The results of this simulation study show that a bigger effect size and sample
size leads to an increased ability to identify the interaction and main effects.&lt;/p&gt;
&lt;p&gt;Furthermore, the different values of neighbors did not have a significant impact on the power. Although, the number of neighbors did have an impact
on the type I error, a higher number of neighbors will result in a lower type
I error. In all the conditions, the false discovery rates are quite high, mean-
ing that of all the selected base-learners, there are still a few uninformative
base-learners selected.  &lt;/p&gt;
&lt;p&gt;Based on this simulation study, it is
strongly recommended to have a sample size of at least 500 and the number
of neighbors set to 10 or the square root of N with repeated cross-validation to identify main
effects and interactions with a SUBiNN model.&lt;/p&gt;</content><category term="Posts"></category><category term="SUBINN"></category><category term="Simulation study"></category><category term="Power"></category></entry><entry><title>Introduction</title><link href="/my-super-post.html" rel="alternate"></link><published>2021-04-12T10:20:00+02:00</published><updated>2010-12-05T19:30:00+01:00</updated><author><name>Mariska van Willigen</name></author><id>tag:None,2021-04-12:/my-super-post.html</id><summary type="html">&lt;p&gt;Who am I?&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hi EVERYONE!!! :) !!!, I wil introduce myself here, blaa blaa blaa My name is Mariska and I might update this blog or maybe never and just make it for fun. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Who knows?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;Mariskas_objects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;b&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;small cute dog&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Very happy Mariska :)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;small cute dog&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Cheesefondue&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;OMGGGGG YESSSSSS :D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Images&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Minion" src="https://octodex.github.com/images/minion.png"&gt;&lt;/p&gt;</content><category term="Posts"></category><category term="First Post"></category><category term="Introduction"></category><category term="Publishing"></category></entry></feed>